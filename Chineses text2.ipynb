{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:38:06.291706800Z",
     "start_time": "2024-02-04T14:37:47.115477700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382688\n",
      "5090680\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "# 文件路径\n",
    "file_path = 'toutiao_cat_data.txt'\n",
    "\n",
    "documents = []\n",
    "all_texts = []\n",
    "\n",
    "# 打开文件并按行处理\n",
    "with open(file_path, encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 使用jieba进行分词\n",
    "        # 提取第三个元素，通过感叹号分割，然后去掉空格并进行分词\n",
    "        words = jieba.cut(line.strip().split('_!_')[3].strip())\n",
    "        document = list(words)\n",
    "\n",
    "        documents.append(document)\n",
    "        all_texts += document\n",
    "\n",
    "# 文档数和总单词数\n",
    "doc_size = len(documents)\n",
    "text_size = len(all_texts)\n",
    "\n",
    "print(doc_size)\n",
    "print(text_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169696\n"
     ]
    },
    {
     "data": {
      "text/plain": "[(0, 299),\n (1, 33),\n (2, 35956),\n (3, 2337),\n (4, 276),\n (5, 1272),\n (6, 10544),\n (7, 12),\n (8, 150717),\n (9, 57)]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = {\n",
    "    '100': '民生故事',\n",
    "    '101': '文化',\n",
    "    '102': '娱乐',\n",
    "    '103': '体育',\n",
    "    '104': '财经',\n",
    "    '106': '房产',\n",
    "    '107': '汽车',\n",
    "    '108': '教育',\n",
    "    '109': '科技',\n",
    "    '110': '军事',\n",
    "    '112': '旅游',\n",
    "    '113': '国际',\n",
    "    '114': '证券股票',\n",
    "    '115': '农业三农',\n",
    "    '116': '电竞游戏'\n",
    "}\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "# 文書データから辞書を作成\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# 全テキストをBoWにすると，各単語の出現数を数えたことになる\n",
    "bow = dictionary.doc2bow(all_texts)\n",
    "\n",
    "# BoWに含まれる単語の種類数（語彙サイズ）は\n",
    "vocab_size = len(bow)\n",
    "print(vocab_size)\n",
    "\n",
    "# 先頭の10単語を眺めてみる（単語IDと頻度）\n",
    "# 自然言語処理の世界では，単語をIDすなわち番号で表す習慣（というより必要性）が昔からあった\n",
    "bow[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:38:10.660900400Z",
     "start_time": "2024-02-04T14:38:06.304749400Z"
    }
   },
   "id": "a044008157465036"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[('之旅', 299),\n ('京城', 33),\n ('你', 35956),\n ('值得', 2337),\n ('博物馆', 276),\n ('文化', 1272),\n ('最', 10544),\n ('来场', 12),\n ('的', 150717),\n ('发酵', 57)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語がID化されているので，単語そのものに戻しておく\n",
    "all_texts_wf = [ (dictionary.get(w[0]), w[1]) for w in dictionary.doc2bow(all_texts) ]\n",
    "\n",
    "# すると先ほどの単語10個は\n",
    "all_texts_wf[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:38:11.311145700Z",
     "start_time": "2024-02-04T14:38:10.660900400Z"
    }
   },
   "id": "8ae6744fc7c7571b"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('，', 267580)\n",
      "('？', 188603)\n",
      "('的', 150717)\n",
      "('！', 70185)\n",
      "('：', 60396)\n",
      "(' ', 53018)\n",
      "('了', 50236)\n",
      "('是', 40195)\n",
      "('你', 35956)\n",
      "('“', 34985)\n",
      "('”', 34898)\n",
      "('吗', 30161)\n",
      "('有', 30114)\n",
      "('在', 26648)\n",
      "('如何', 22601)\n",
      "('什么', 21552)\n",
      "('怎么', 20168)\n",
      "('为什么', 19554)\n",
      "('和', 18842)\n",
      "('都', 17407)\n",
      "('中国', 17050)\n",
      "('》', 16557)\n",
      "('《', 16546)\n",
      "('被', 16408)\n",
      "('对', 16034)\n",
      "('不', 15898)\n",
      "('人', 15423)\n",
      "('、', 13982)\n",
      "('会', 13724)\n",
      "('我', 13421)\n",
      "('看', 12626)\n",
      "('年', 12450)\n",
      "('这', 11749)\n",
      "('美国', 10913)\n",
      "('能', 10698)\n",
      "('上', 10585)\n",
      "('最', 10544)\n",
      "('上联', 10138)\n",
      "('好', 10042)\n",
      "('下联', 9987)\n",
      "('要', 9681)\n",
      "('还', 9643)\n",
      "('后', 9468)\n",
      "('大', 9278)\n",
      "('就', 9238)\n",
      "('5', 8818)\n",
      "('中', 8803)\n",
      "('哪些', 8776)\n",
      "('将', 8678)\n",
      "('与', 8637)\n"
     ]
    }
   ],
   "source": [
    "# 頻度を降順に並べ替えて眺めてみる\n",
    "all_texts_wf_sorted = sorted(all_texts_wf, key=lambda wf: wf[1], reverse=True)\n",
    "for i in range(50):\n",
    "    print(all_texts_wf_sorted[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:38:11.343145300Z",
     "start_time": "2024-02-04T14:38:11.333145500Z"
    }
   },
   "id": "bee36c84654c1b47"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 之旅:1 京城:1 你:1 值得:1 博物馆:1 文化:1 最:1 来场:1 的:1\n"
     ]
    }
   ],
   "source": [
    "# 今度は文書ごとにBoWにする\n",
    "doc_bow = [dictionary.doc2bow(d) for d in documents]\n",
    "\n",
    "# 保存もできる\n",
    "corpora.MmCorpus.serialize('diet/diet.mm', doc_bow)\n",
    "# ある1つの文書の単語頻度を今度は昇順に並べ替えて，眺めてみる\n",
    "# たとえば最初（0番目）の文書\n",
    "doc_id = 0\n",
    "\n",
    "wv = [(dictionary.get(w[0]), w[1]) for w in sorted(doc_bow[doc_id], key=lambda wf: wf[1])]\n",
    "\n",
    "# この文書の単語の種類数\n",
    "# 文書に出現した単語しか含まれない（したがって全体の語彙サイズより小さい）ことに注意\n",
    "\n",
    "# スペースの節約のため，つなげてから表示\n",
    "wv_seq = \"\"\n",
    "for i in range(len(wv)):\n",
    "    # 単語の後にコロン（：）で区切って頻度を付けて表示\n",
    "    wv_seq += \" \" + wv[i][0] + \":\" + str(wv[i][1])\n",
    "print(wv_seq)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:38:16.726829Z",
     "start_time": "2024-02-04T14:38:11.391145800Z"
    }
   },
   "id": "bd80eae9da06011d"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 242. GiB for an array with shape (64940622848,) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-26-d3780ff64972>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;31m# 空（0）の行列を用意して，各文書で単語が出現していた場合だけその頻度を代入\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mword_vect\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc_size\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mvocab_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mint32\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvocab_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdoc_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mw\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdoc_bow\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 242. GiB for an array with shape (64940622848,) and data type int32"
     ]
    }
   ],
   "source": [
    "# データフレームに変換する場合\n",
    "import numpy as np\n",
    "from pandas import DataFrame \n",
    "\n",
    "# 空（0）の行列を用意して，各文書で単語が出現していた場合だけその頻度を代入\n",
    "word_vect = np.zeros(doc_size * vocab_size, dtype=np.int32).reshape(doc_size, vocab_size)\n",
    "for i in range(doc_size):\n",
    "    for w, f in doc_bow[i]:\n",
    "        word_vect[i][w] = f\n",
    "\n",
    "# データフレームに変換して列名に単語を設定\n",
    "word_df = DataFrame(word_vect)\n",
    "word_df.columns=[ dictionary.get(i) for i in range(vocab_size)]\n",
    "word_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:38:16.745283800Z",
     "start_time": "2024-02-04T14:38:16.722829300Z"
    }
   },
   "id": "c529f8174b0f6519"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# この単語頻度ベクトルがそれぞれの文書を表しているので，これにより分類などを行うことが（いちおう）できる\n",
    "\n",
    "# 読点「、」と句点「。」がどの文書でも著しく頻度が大きいので，あらかじめ抜いておく\n",
    "del word_df['、']\n",
    "del word_df['。']\n",
    "\n",
    "word_vect = np.array(word_df)\n",
    "\n",
    "\n",
    "# コサイン類似度を定義して\n",
    "def cos_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / np.linalg.norm(v1) / np.linalg.norm(v2)\n",
    "\n",
    "\n",
    "# 文書間の類似度を計算\n",
    "wv_sims = [[cos_similarity(i, j) for j in word_vect] for i in word_vect]\n",
    "# たとえば最初の文書と各文書との類似度を見ると，どれもあまり変わらない…\n",
    "# （これは，句読点は外したが，どの文書にも出てくる機能語などはそのままにしたため）\n",
    "wv_sims[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T14:38:16.740284500Z"
    }
   },
   "id": "b7440df2ecdae9c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TF-IDF\n",
    "from gensim import models\n",
    "\n",
    "# 読み込んだ文書データからIDFを計算\n",
    "tfidf = models.TfidfModel(doc_bow)\n",
    "\n",
    "# これをBoWのデータ（つまりTF）に適用するとTF-IDFが計算できる\n",
    "# 同じ文書集合で計算してみる（もちろん別の文書でもできる）\n",
    "tfidf_vect = tfidf[doc_bow]\n",
    "\n",
    "# たとえば最初の文書について，先ほどのBoWとTF-IDFとで単語の種類数を見てみると\n",
    "print(len(doc_bow[0]))\n",
    "print(len(tfidf_vect[0]))\n",
    "\n",
    "# 大きさが異なる（TF-IDFの種類の方が少ない）ことに注意\n",
    "# これは，TF-IDFの計算時に，全文書に出てくる単語が（TF-IDFが0になるので）省かれるため"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T14:38:16.741284100Z"
    }
   },
   "id": "56498b1edc9e8294"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ある1つの文書の単語をいくつか抜き出してTF-IDF値を見てみる\n",
    "# （値が正規化されていることに注意）\n",
    "# たとえば最初（0番目）の文書\n",
    "doc_id = 0\n",
    "\n",
    "for i in range(0, 3000, 50):\n",
    "    print(\"%-8s\\t%.6f\" % (dictionary.get(tfidf_vect[doc_id][i][0]), tfidf_vect[doc_id][i][1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T14:38:16.742284100Z"
    }
   },
   "id": "ade1400730d13e77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TF-IDFによる文書のベクトルが定義できたので，類似度を計算してみる\n",
    "from gensim import similarities\n",
    "\n",
    "index = similarities.MatrixSimilarity(tfidf_vect)\n",
    "\n",
    "# 同じ文書集合に対する類似度（他の文書を用意して計算することも，もちろんできる）\n",
    "sims = index[tfidf_vect]\n",
    "# 最初の文書に関する類似度\n",
    "print(sims[0])\n",
    "\n",
    "# 単純な単語頻度ベクトルよりはメリハリがついた"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T14:38:16.743284400Z"
    }
   },
   "id": "e2d130d72cfa0c0d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Document {i + 1}: {documents[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T14:38:16.744284100Z"
    }
   },
   "id": "1e0df640a805f243"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 最初の辞書から，文書頻度が一定以下・一定以上のものを除去\n",
    "documents = documents[:10000]\n",
    "dictionary.filter_extremes()\n",
    "\n",
    "# 新しい辞書のサイズ\n",
    "print(len(list(dictionary.values())))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-04T14:38:16.745283800Z"
    }
   },
   "id": "89ce0f1b2fd47861"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-30-5c9b8dbaf298>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mtopic_words\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mdocuments\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdocuments\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m10000\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mdocuments\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mre\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msub\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr'[^\\w\\s]'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m''\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdocuments\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[0mtopic_num\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mword_vect2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m \u001B[0mdictionary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdoc2bow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0md\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdocuments\u001B[0m \u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Tool\\Anaconda\\envs\\Tensorflow\\lib\\re.py\u001B[0m in \u001B[0;36msub\u001B[1;34m(pattern, repl, string, count, flags)\u001B[0m\n\u001B[0;32m    192\u001B[0m     \u001B[0ma\u001B[0m \u001B[0mcallable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mit\u001B[0m\u001B[0;31m'\u001B[0m\u001B[0ms\u001B[0m \u001B[0mpassed\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mMatch\u001B[0m \u001B[0mobject\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mmust\u001B[0m \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    193\u001B[0m     a replacement string to be used.\"\"\"\n\u001B[1;32m--> 194\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_compile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpattern\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msub\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrepl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstring\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcount\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    195\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0msubn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpattern\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrepl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstring\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcount\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mflags\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# LDAで学習された各トピックを特徴付ける単語（それぞれ重み，すなわち確率を持っている）\n",
    "# クラスタリングにおけるクラスタの番号とは全く関係がないので注意\n",
    "import re\n",
    "topic_words = []\n",
    "documents = documents[:10000]\n",
    "documents=re.sub(r'[^\\w\\s]', '', documents)\n",
    "topic_num=10\n",
    "word_vect2 = [ dictionary.doc2bow(d) for d in documents ]\n",
    "lda = models.LdaModel(word_vect2, num_topics=topic_num, passes=10)\n",
    "\n",
    "for i in range(topic_num):\n",
    "    topic_words.append(\" \".join([ \"%s@%.3f\" % (dictionary.get(t[0]), t[1]) for t in lda.get_topic_terms(i) ] ))\n",
    "    print(\"%d: %s\" % (i, topic_words[-1]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:46:00.004047400Z",
     "start_time": "2024-02-04T14:45:59.997046700Z"
    }
   },
   "id": "dbcfc97dc11db064"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 的@0.000 是@0.000 了@0.000 有@0.000 你@0.000 怎么@0.000 吗@0.000 在@0.000 什么@0.000 如何@0.000\n",
      "1: 的@0.001 是@0.000 吗@0.000 了@0.000 你@0.000 怎么@0.000 在@0.000 有@0.000 什么@0.000 如何@0.000\n",
      "2: 的@0.002 了@0.001 是@0.001 在@0.001 你@0.001 有@0.001 什么@0.001 吗@0.000 如何@0.000 怎么@0.000\n",
      "3: 的@0.036 了@0.011 你@0.010 是@0.009 吗@0.008 有@0.008 在@0.007 怎么@0.007 什么@0.006 如何@0.006\n",
      "4: 的@0.002 了@0.001 你@0.001 如何@0.001 吗@0.001 是@0.001 有@0.001 怎么@0.000 什么@0.000 在@0.000\n",
      "5: 的@0.026 了@0.008 有@0.007 是@0.006 吗@0.006 你@0.006 怎么@0.005 为什么@0.005 在@0.004 什么@0.004\n",
      "6: 的@0.000 了@0.000 你@0.000 吗@0.000 有@0.000 是@0.000 在@0.000 怎么@0.000 如何@0.000 为什么@0.000\n",
      "7: 的@0.001 吗@0.000 你@0.000 是@0.000 了@0.000 怎么@0.000 在@0.000 有@0.000 为什么@0.000 如何@0.000\n",
      "8: 的@0.001 你@0.000 有@0.000 吗@0.000 了@0.000 是@0.000 怎么@0.000 如何@0.000 和@0.000 什么@0.000\n",
      "9: 的@0.001 了@0.000 吗@0.000 有@0.000 你@0.000 怎么@0.000 是@0.000 在@0.000 什么@0.000 和@0.000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from gensim import corpora, models\n",
    "\n",
    "documents = [str(sentence) for sentence in documents]\n",
    "\n",
    "# 将文档列表转换为字符串\n",
    "documents_text = \" \".join(documents)\n",
    "\n",
    "# 使用正则表达式移除标点符号\n",
    "documents_text = re.sub(r'[^\\w\\s]', '', documents_text)\n",
    "\n",
    "# 将处理后的文本拆分为单词\n",
    "preprocessed_documents = documents_text.split()\n",
    "\n",
    "# 创建字典和文档向量\n",
    "dictionary = corpora.Dictionary([preprocessed_documents])\n",
    "word_vect2 = [dictionary.doc2bow(preprocessed_documents)]\n",
    "\n",
    "# 使用LDA模型\n",
    "topic_num = 10\n",
    "lda = models.LdaModel(word_vect2, num_topics=topic_num, passes=10)\n",
    "\n",
    "# 打印主题词\n",
    "topic_words = []\n",
    "for i in range(topic_num):\n",
    "    topic_words.append(\" \".join([\"%s@%.3f\" % (dictionary.get(t[0]), t[1]) for t in lda.get_topic_terms(i)]))\n",
    "    print(\"%d: %s\" % (i, topic_words[-1]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:50:33.866305700Z",
     "start_time": "2024-02-04T14:50:33.127465300Z"
    }
   },
   "id": "593fed30b589a966"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\64403\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T14:54:35.136834600Z",
     "start_time": "2024-02-04T14:54:33.900983300Z"
    }
   },
   "id": "3b16f15d0450ccfd"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 中国@0.001 对@0.001 会@0.001 好@0.000 我@0.000 美国@0.000 能@0.000 大@0.000 上@0.000 还@0.000\n",
      "1: 对@0.001 我@0.001 中国@0.001 会@0.001 好@0.000 被@0.000 看@0.000 就@0.000 能@0.000 最@0.000\n",
      "2: 被@0.002 年@0.002 中国@0.002 对@0.002 看@0.001 我@0.001 会@0.001 月@0.001 最@0.001 还@0.001\n",
      "3: 被@0.001 中国@0.001 对@0.001 看@0.001 年@0.001 美国@0.001 要@0.001 会@0.001 能@0.001 上@0.001\n",
      "4: 对@0.001 被@0.001 中国@0.001 会@0.001 我@0.001 上@0.001 美国@0.001 最@0.001 还@0.001 5@0.001\n",
      "5: 被@0.001 对@0.001 中国@0.001 我@0.001 会@0.001 上@0.001 好@0.001 手机@0.001 看@0.001 后@0.001\n",
      "6: 会@0.001 年@0.001 中国@0.001 能@0.000 5@0.000 对@0.000 最@0.000 被@0.000 看@0.000 美国@0.000\n",
      "7: 被@0.001 会@0.001 中国@0.001 对@0.001 我@0.001 年@0.001 看@0.001 美国@0.001 上@0.001 后@0.001\n",
      "8: 中国@0.002 被@0.002 会@0.002 对@0.002 上@0.001 我@0.001 年@0.001 手机@0.001 大@0.001 看@0.001\n",
      "9: 中国@0.005 被@0.005 对@0.005 会@0.004 我@0.004 看@0.004 年@0.004 美国@0.003 能@0.003 上@0.003\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from gensim import corpora, models\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "file_path = 'toutiao_cat_data.txt'\n",
    "\n",
    "documents = []\n",
    "all_texts = []\n",
    "\n",
    "# 打开文件并按行处理\n",
    "with open(file_path, encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # 使用jieba进行分词\n",
    "        # 提取第三个元素，通过感叹号分割，然后去掉空格并进行分词\n",
    "        words = jieba.cut(line.strip().split('_!_')[3].strip())\n",
    "        document = list(words)\n",
    "\n",
    "        documents.append(document)\n",
    "\n",
    "# 将每个句子转换为字符串\n",
    "documents = [str(sentence) for sentence in documents]\n",
    "\n",
    "# 将文档列表转换为字符串\n",
    "documents_text = \" \".join(documents)\n",
    "\n",
    "# 使用正则表达式移除标点符号\n",
    "documents_text = re.sub(r'[^\\w\\s]', '', documents_text)\n",
    "\n",
    "# 将处理后的文本拆分为单词\n",
    "preprocessed_documents = documents_text.split()\n",
    "\n",
    "# 合并停用词列表\n",
    "stop_words = ['的', '是', '在', '了', '和', '与', '及', '他', '她', '它', '我们', '你们', '自己', '这', '那', '什么', '为什么', '如何', '怎么', '哪些', '上联', '哪个', '应该', '多少', '一个', '下联', '还是', '如果', '为何', '怎样', '到底', '不能', '知道', '就是', '影响', '怎么样', '哪里', '这个', '的', '是','你','有','吗','人','都','不']\n",
    "\n",
    "# 移除停用词\n",
    "preprocessed_documents = [word for word in preprocessed_documents if word.lower() not in stop_words]\n",
    "\n",
    "\n",
    "# 创建字典和文档向量\n",
    "dictionary = corpora.Dictionary([preprocessed_documents])\n",
    "word_vect2 = [dictionary.doc2bow(preprocessed_documents)]\n",
    "\n",
    "# 使用LDA模型\n",
    "topic_num = 10\n",
    "lda = models.LdaModel(word_vect2, num_topics=topic_num, passes=10)\n",
    "\n",
    "# 打印主题词\n",
    "topic_words = []\n",
    "for i in range(topic_num):\n",
    "    topic_words.append(\" \".join([\"%s@%.3f\" % (dictionary.get(t[0]), t[1]) for t in lda.get_topic_terms(i)]))\n",
    "    print(\"%d: %s\" % (i, topic_words[-1]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T15:06:01.922649Z",
     "start_time": "2024-02-04T15:05:35.133727Z"
    }
   },
   "id": "a17b583596b403d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
